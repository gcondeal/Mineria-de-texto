{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica minería de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Presentación](#opcion-1)\n",
    "* [Técnicas utilizadas](#opcion-2)\n",
    "    * [Limpieza de texto](#opcion-2-1)\n",
    "    * [Stemming](#opcion-2-2)\n",
    "    * [Lematización](#opcion-2-3)\n",
    "    * [Entidades nombradas(*ENR*)](#opcion-2-4)\n",
    "* [Solución](#opcion-3)\n",
    "    * [Extracción de la información](#opcion-3-1)\n",
    "    * [Tratamiento de los textos](#opcion-3-2)\n",
    "    * [Agrupación y resultado](#opcion-3-3)\n",
    "* [Conclusiones](#opcion-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentación <a class=\"anchor\" id=\"opcion-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejercicio consiste en el tratamiento de una serie de noticias, descargadas de distintos diarios electrónicos, y que deben se agrupadas de forma automática según su contenido.\n",
    "\n",
    "Para ello hay que hacer uso de distintas técnicas de tratamiento de lenguaje natural: limpieza del texto, stemming, lematizado, tratamiento de entiidades nombradas, etc.\n",
    "\n",
    "Se parte de un código fuente en el que teniendo disponibles los fichero .txt, se encarga de realizar las agrupación de los textos y compararlo con un array de valores en el que se representa el resultado ideal.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas utilizadas <a class=\"anchor\" id=\"opcion-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización del ejercicio se han utilizados las siguientes técnicas de tratamiento de lenguaje natural, haciendo uso de la librería *Python* **NLTK**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Limpieza del texto: <a class=\"anchor\" id=\"opcion-2-1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se elimina del texto original los signos de puntuación y aquellas palabras que no aportarán información a los pasos posteriores, denominadas *stopwords*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stemming: <a class=\"anchor\" id=\"opcion-2-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un método para reducir una palabra a su raíz, haciendo que un tratamiento de búsqueda o agrupación posterior considere dos palabras que tienen esa raíz común como la misma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Lematización: <a class=\"anchor\" id=\"opcion-2-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Método por el cual se simplifica una *forma flexionada* (plural, femenino, conjugada, ...) y sea sustituida por la forma que por norma es aceptada como representación de todas ellas, es decir, la forma que podríamos encontrar en cualquier diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Entidades nombradas (*ENR*): <a class=\"anchor\" id=\"opcion-2-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Unidad de información fundamental que se refiere a nombres propios que pueden ser clasificados en categorías variadas.\n",
    "    \n",
    "   Las principales categorías son: Personas, Lugares y Organizaciones, si bien pueden aparecer mas segun el tipo de dato (fechas) o el dominio del texto (político, farmacéutico, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución <a class=\"anchor\" id=\"opcion-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Extración de la información <a class=\"anchor\" id=\"opcion-3-1\"></a>\n",
    "    \n",
    "Como primera acción se ha realizado un proceso de extracción de la información desde los fichero *HTML* a *txt*.\n",
    "Haciendo uso de la librería *BeautifulSoup* se han realizan los siguientes pasos:\n",
    "- Identificamos el origen. dado que los fichero están descargado y no tenemosla URL, se hace uso de la metainformación guarda en el propio HTML haciendo referencia al origen del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "    origen = bsObj.find(text=lambda text:isinstance(text, Comment))\n",
    "    if \"saved from url\" in origen: # puedo identificar desde donde se ha descargado la página\n",
    "        ....\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Según el origen se identifican los *tag's* *html* que contienen tanto el titular de la noticia, para nombrar el *txt* resultante como el cuerpo de la noticia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if \"www.theguardian.com\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'class' : 'content__headline'}).text\n",
    "        objBody = bsObj.find('div', attrs={'itemprop' : 'articleBody'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en algún caso, además del titular, se extrae una segunda cabecera de la noticia, o se elimina información sobrante que la librería extrae junto con el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    elif \"www.telegraph.co.uk\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'itemprop' : 'headline name'}).text\n",
    "        objBody = bsObj.find('article', attrs={'itemprop': 'articleBody'})\n",
    "        cad_inicio = '/* dynamic basic css */'\n",
    "        cad_fin = 'OBR.extern.researchWidget();'\n",
    "    elif \"elpais.com\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'itemprop': 'headline'}).text\n",
    "        subtitulo = bsObj.find('h2', attrs={'itemprop': 'alternativeHeadline'}).text\n",
    "        objBody = bsObj.find('div', attrs={'itemprop': 'articleBody'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- con la información recogida se guarda en una nueva carpeta las conversiones a *txt* de cada ficheros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    f2 = open(folderDestino + \"/\" + titulo + \".txt\", \"w\")\n",
    "    f2.write(titulo + \"\\n\")\n",
    "\n",
    "    if subtitulo != None:\n",
    "        subtitulo = subtitulo.replace(\"\\n\", \"\")\n",
    "        f2.write(subtitulo + \"\\n\")\n",
    "\n",
    "    if objBody != None:\n",
    "        for parrafo in objBody.findAll('p'):\n",
    "            aux = parrafo.text\n",
    "\n",
    "            if cad_inicio != None and cad_fin != None:\n",
    "\n",
    "                pos_inicio = aux.find(cad_inicio)\n",
    "                pos_fin = aux.find(cad_fin) + len(cad_fin)\n",
    "\n",
    "                if pos_inicio != -1 or pos_fin != -1:\n",
    "                    aux = aux[:pos_inicio] + aux[pos_fin:]\n",
    "\n",
    "            f2.write(aux + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Lectura de los *txt* <a class=\"anchor\" id=\"opcion-3-2\"></a>\n",
    "\n",
    "Se recoge cada uno de los *txt's* generados en el punto anterior y se cargan en memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    listing = os.listdir(folder + \"/txt\")\n",
    "    for file in listing:\n",
    "\n",
    "        if file.endswith(\".txt\"):\n",
    "            url = folder+\"/txt/\"+file\n",
    "            f = open(url,encoding=\"ANSI\");\n",
    "            raw = f.read()\n",
    "            f.close()\n",
    "            t = TextBlob(raw)\n",
    "            idioma = t.detect_language()\n",
    "            print(\"File: \", file,\" escrito en: \", idioma)\n",
    "            \n",
    "            raw_limpio = limpia_signos_puntuacion(raw)\n",
    "            tokens = nltk.word_tokenize(raw_limpio)\n",
    "            tokens_limpio = limpia_stop_words(tokens, idioma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tratamiento de los textos <a class=\"anchor\" id=\"opcion-3-3\"></a>\n",
    "\n",
    "Sobre los texto leídos se aplican los distintos métodos y herramientas descritos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #text = nltk.Text(stemming(tokens_limpio, idioma))\n",
    "\n",
    "        #text = nltk.Text(lemmatization(tokens_limpio, idioma))\n",
    "\n",
    "        #text = nltk.Text(stemming(lemmatization(tokens_limpio, idioma),idioma))\n",
    "\n",
    "        #text = nltk.Text(trata_entity_names(tokens_limpio))\n",
    "\n",
    "        #text = nltk.Text(stemming(trata_entity_names(tokens_limpio), idioma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Agrupación y resultado <a class=\"anchor\" id=\"opcion-3-2\"></a>\n",
    "\n",
    "Tras haber aplicado cada uno de los métodos, se procesde a realizar la agrupación y la comparación con el array de soluciones optimo, obteniendo el porcentaje de exito del proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    distanceFunction =\"cosine\"\n",
    "    #distanceFunction = \"euclidean\"\n",
    "    test = cluster_texts(texts,5,distanceFunction)\n",
    "    print(\"test: \", test)\n",
    "    # Gold Standard\n",
    "    reference =[0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "    print(\"reference: \", reference)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"rand_score: \", adjusted_rand_score(reference,test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones <a class=\"anchor\" id=\"opcion-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejando aparte el proceso de extracción del texto desde el HTML, los pasos dados para conseguir la mejor agrupación han sido:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- realizando una primera ejecución del proceso sin modificar el texto leído, obtenemos el siguiente resultado, siendo este el punto de partida y el que obtendrá la peor puntuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - sobre el texto leído desde el *txt* se eliminan los signos de puntuación y las *stopwords*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

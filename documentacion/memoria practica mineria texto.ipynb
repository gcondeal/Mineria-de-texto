{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica minería de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Presentación](#opcion-1)\n",
    "* [Técnicas utilizadas](#opcion-2)\n",
    "    * [Limpieza de texto](#opcion-2-1)\n",
    "    * [Stemming](#opcion-2-2)\n",
    "    * [Lematización](#opcion-2-3)\n",
    "    * [Entidades nombradas(*ENR*)](#opcion-2-4)\n",
    "* [Solución](#opcion-3)\n",
    "    * [Extracción de la información](#opcion-3-1)\n",
    "    * [Tratamiento de los textos](#opcion-3-2)\n",
    "    * [Agrupación y resultado](#opcion-3-3)\n",
    "* [Conclusiones](#opcion-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentación <a class=\"anchor\" id=\"opcion-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejercicio consiste en el tratamiento de una serie de noticias, descargadas de distintos diarios electrónicos, y que deben se agrupadas de forma automática según su contenido.\n",
    "\n",
    "Para ello hay que hacer uso de distintas técnicas de tratamiento de lenguaje natural: limpieza del texto, stemming, lematizado, tratamiento de entiidades nombradas, etc.\n",
    "\n",
    "Se parte de un código fuente en el que teniendo disponibles los fichero .txt, se encarga de realizar las agrupación de los textos y compararlo con un array de valores en el que se representa el resultado ideal.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Técnicas utilizadas <a class=\"anchor\" id=\"opcion-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la realización del ejercicio se han utilizados las siguientes técnicas de tratamiento de lenguaje natural, haciendo uso de la librería *Python* **NLTK**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Limpieza del texto: <a class=\"anchor\" id=\"opcion-2-1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso se elimina del texto original los signos de puntuación y aquellas palabras que no aportarán información a los pasos posteriores, denominadas *stopwords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpia_signos_puntuacion(texto):\n",
    "    text_limpio = ''\n",
    "    for letter in texto:\n",
    "        if not letter in string.punctuation:\n",
    "            text_limpio = text_limpio + letter\n",
    "\n",
    "    return text_limpio\n",
    "\n",
    "def limpia_stop_words(tokens, idioma):\n",
    "    if idioma == 'en':\n",
    "        stopWords = nltk.corpus.stopwords.words('english')\n",
    "    elif idioma == 'es':\n",
    "        stopWords = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "    return [w for w in tokens if w.lower() not in stopWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stemming: <a class=\"anchor\" id=\"opcion-2-2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un método para reducir una palabra a su raíz, haciendo que un tratamiento de búsqueda o agrupación posterior considere dos palabras que tienen esa raíz común como la misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens, idioma):\n",
    "    # Seleccionamos el steamer que deseados utilizar.\n",
    "    if idioma == 'en':\n",
    "        stemmer = PorterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "    stemmeds = []\n",
    "\n",
    "    # Para cada token del texto obtenemos su raíz.\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        stemmeds.append(stemmed)\n",
    "\n",
    "    return stemmeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Lematización: <a class=\"anchor\" id=\"opcion-2-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Método por el cual se simplifica una *forma flexionada* (plural, femenino, conjugada, ...) y sea sustituida por la forma que por norma es aceptada como representación de todas ellas, es decir, la forma que podríamos encontrar en cualquier diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_value(value):\n",
    "    result = ''\n",
    "    # Filtramos las palabras y nos quedamos solo las que nos pueden interesar.\n",
    "    # Estas son Adjetivos, Verbos, Sustantivos y Adverbios.\n",
    "    if value.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif value.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif value.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif value.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return result\n",
    "\n",
    "def lemmatization(tokens, idioma):\n",
    "    if idioma != 'en':\n",
    "        return tokens\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens_aux = nltk.pos_tag(tokens)\n",
    "    lemmatizeds = []\n",
    "\n",
    "    for token in tokens_aux:\n",
    "        if len(token) > 0:\n",
    "            pos = wordnet_value(token[1])\n",
    "            # Filtramos las palabras que no nos interesan.\n",
    "            if pos != '':\n",
    "                lemmatizeds.append(wordnet_lemmatizer.lemmatize(str(token[0]).lower(), pos=pos))\n",
    "\n",
    "    return lemmatizeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Entidades nombradas (*ENR*): <a class=\"anchor\" id=\"opcion-2-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Unidad de información fundamental que se refiere a nombres propios que pueden ser clasificados en categorías variadas.\n",
    "    \n",
    "   Las principales categorías son: Personas, Lugares y Organizaciones, si bien pueden aparecer mas segun el tipo de dato (fechas) o el dominio del texto (político, farmacéutico, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrae_entity_names(t):\n",
    "    entity_names = []\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extrae_entity_names(child))\n",
    "    return entity_names\n",
    "\n",
    "def trata_entity_names(tokens):\n",
    "    tagged_sentences = [nltk.pos_tag(tokens)]\n",
    "    chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "    entity_names = []\n",
    "    for tree in chunked_sentences:\n",
    "        entity_names.extend(extrae_entity_names(tree))\n",
    "\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución <a class=\"anchor\" id=\"opcion-3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Extración de la información <a class=\"anchor\" id=\"opcion-3-1\"></a>\n",
    "    \n",
    "Como primera acción se ha realizado un proceso de extracción de la información desde los fichero *HTML* a *txt*.\n",
    "Haciendo uso de la librería *BeautifulSoup* se han realizan los siguientes pasos:\n",
    "- Identificamos el origen. dado que los fichero están descargado y no tenemosla URL, se hace uso de la metainformación guarda en el propio HTML haciendo referencia al origen del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "    origen = bsObj.find(text=lambda text:isinstance(text, Comment))\n",
    "    if \"saved from url\" in origen: # puedo identificar desde donde se ha descargado la página\n",
    "        ....\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Según el origen se identifican los *tag's* *html* que contienen tanto el titular de la noticia, para nombrar el *txt* resultante como el cuerpo de la noticia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if \"www.theguardian.com\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'class' : 'content__headline'}).text\n",
    "        objBody = bsObj.find('div', attrs={'itemprop' : 'articleBody'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en algún caso, además del titular, se extrae una segunda cabecera de la noticia, o se elimina información sobrante que la librería extrae junto con el texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    elif \"www.telegraph.co.uk\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'itemprop' : 'headline name'}).text\n",
    "        objBody = bsObj.find('article', attrs={'itemprop': 'articleBody'})\n",
    "        cad_inicio = '/* dynamic basic css */'\n",
    "        cad_fin = 'OBR.extern.researchWidget();'\n",
    "    elif \"elpais.com\" in origen:\n",
    "        hayQueTratar = True\n",
    "        titulo = bsObj.find('h1', attrs={'itemprop': 'headline'}).text\n",
    "        subtitulo = bsObj.find('h2', attrs={'itemprop': 'alternativeHeadline'}).text\n",
    "        objBody = bsObj.find('div', attrs={'itemprop': 'articleBody'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- con la información recogida se guarda en una nueva carpeta las conversiones a *txt* de cada ficheros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    f2 = open(folderDestino + \"/\" + titulo + \".txt\", \"w\")\n",
    "    f2.write(titulo + \"\\n\")\n",
    "\n",
    "    if subtitulo != None:\n",
    "        subtitulo = subtitulo.replace(\"\\n\", \"\")\n",
    "        f2.write(subtitulo + \"\\n\")\n",
    "\n",
    "    if objBody != None:\n",
    "        for parrafo in objBody.findAll('p'):\n",
    "            aux = parrafo.text\n",
    "\n",
    "            if cad_inicio != None and cad_fin != None:\n",
    "\n",
    "                pos_inicio = aux.find(cad_inicio)\n",
    "                pos_fin = aux.find(cad_fin) + len(cad_fin)\n",
    "\n",
    "                if pos_inicio != -1 or pos_fin != -1:\n",
    "                    aux = aux[:pos_inicio] + aux[pos_fin:]\n",
    "\n",
    "            f2.write(aux + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Lectura de los *txt* <a class=\"anchor\" id=\"opcion-3-2\"></a>\n",
    "\n",
    "Se recoge cada uno de los *txt's* generados en el punto anterior y se cargan en memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    listing = os.listdir(folder + \"/txt\")\n",
    "    for file in listing:\n",
    "\n",
    "        if file.endswith(\".txt\"):\n",
    "            url = folder+\"/txt/\"+file\n",
    "            f = open(url,encoding=\"ANSI\");\n",
    "            raw = f.read()\n",
    "            f.close()\n",
    "            t = TextBlob(raw)\n",
    "            idioma = t.detect_language()\n",
    "            print(\"File: \", file,\" escrito en: \", idioma)\n",
    "            \n",
    "            raw_limpio = limpia_signos_puntuacion(raw)\n",
    "            tokens = nltk.word_tokenize(raw_limpio)\n",
    "            tokens_limpio = limpia_stop_words(tokens, idioma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tratamiento de los textos <a class=\"anchor\" id=\"opcion-3-3\"></a>\n",
    "\n",
    "Sobre los texto leídos se aplican los distintos métodos y herramientas descritos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #text = nltk.Text(stemming(tokens_limpio, idioma))\n",
    "\n",
    "        #text = nltk.Text(lemmatization(tokens_limpio, idioma))\n",
    "\n",
    "        #text = nltk.Text(stemming(lemmatization(tokens_limpio, idioma),idioma))\n",
    "\n",
    "        #text = nltk.Text(trata_entity_names(tokens_limpio))\n",
    "\n",
    "        #text = nltk.Text(stemming(trata_entity_names(tokens_limpio), idioma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Agrupación y resultado <a class=\"anchor\" id=\"opcion-3-2\"></a>\n",
    "\n",
    "Tras haber aplicado cada uno de los métodos, se procesde a realizar la agrupación y la comparación con el array de soluciones optimo, obteniendo el porcentaje de exito del proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    distanceFunction =\"cosine\"\n",
    "    #distanceFunction = \"euclidean\"\n",
    "    test = cluster_texts(texts,5,distanceFunction)\n",
    "    print(\"test: \", test)\n",
    "    # Gold Standard\n",
    "    reference =[0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "    print(\"reference: \", reference)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"rand_score: \", adjusted_rand_score(reference,test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones <a class=\"anchor\" id=\"opcion-4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dejando aparte el proceso de extracción del texto desde el HTML, los pasos dados para conseguir la mejor agrupación han sido:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- realizando una primera ejecución del proceso sin modificar el texto leído, obtenemos el siguiente resultado, siendo este el punto de partida y el que obtendrá la peor puntuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [1  0  1  4  1  1  1  4  4  0  0  0  4  1  3  4  1  1  1  2  1  4]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.151515151515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - sobre el texto leído desde el *txt* se eliminan los signos de puntuación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [1  0  1  3  1  1  1  3  3  0  0  0  3  1  4  3  1  1  1  2  1  3]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.151515151515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y las *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [0  3  0  1  0  0  0  4  2  3  3  3  2  0  0  1  0  0  0  0  0  2]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.209205020921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso la eliminación de los signos de puntuación no influye en el resultado, siendo un cambio más defacilidad para poder aplicar otros métodos que  algo que pueda afectar a un cambio en la agrupación de palabras.\n",
    "La eliminación de las *stopwords* sí influye debido a que se está eliminando palabras que no aportan información pero sí puede  equivocar al proceso de agrupación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aplicamos stemming, sin cambios en el resultado, ya que en este tipo de textos las formas gramaticales usadas no son my variadas y tampoco influyen en lo que realmente es el asunto principal del mismo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [0  3  0  1  0  0  0  4  2  3  3  3  2  0  0  1  0  0  0  0  0  2]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.209205020921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aplicamos lematización, con identico resultado, por el mismo motivo que con el steamming. En este caso sólo se aplica a los documentos en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [0  3  0  1  0  0  0  4  2  3  3  3  2  0  0  1  0  0  0  0  0  2]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.209205020921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- probamos a combinar ambas técnicas por si pudiese verse alguna mejora, pero se obtiene el mismo resultado que de forma individual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [0  3  0  1  0  0  0  4  2  3  3  3  2  0  0  1  0  0  0  0  0  2]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.209205020921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- haciendo uso de las entidades nombradas si se ve un avance realmente considerable en el resultado. esto es debido a que en este tipo de texto, son precisamente esas pocas lpalabras las que definen el texto completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [3  0  3  3  3  2  2  4  0  0  0  0  0  1  1  1  1  3  2  3  2  0]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.918604651163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aplicamos stemmin sobre la lista de palabras extraidas al aplicar las entidades nombradas. Sorprendentemente el resultado empeora considerablemente. Esto se debe a que al modificar una lista de palabras tan reducida hace que al proceso de clusterización le custe más identificar el grupo de cada noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [0  4  0  0  0  3  3  0  2  4  4  4  2  1  1  1  1  0  3  0  3  2]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.685714285714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las pruebas realizadas hasta el momento, con la aplicación de las ENR se obtiene el mejor resultado, produciéndose únicamente un error.\n",
    "Todas estas pruebas se han realizado con los docuemnto en dos idiomas: inglés y español, vamos a repetir las pruebas traduciendo los texto en español al inglés, que tiene la herramientas más depuradas para el tratamiento del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- la primera pruab sin signos de puntuación u sin *stopwords* parece bastante prometedor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [4  1  4  4  0  0  0  2  1  1  1  1  1  3  3  3  3  0  0  0  0  1]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.722117202268"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evidentemente el intentar agrupar textos en distintos idiomas hace el proceso más complicado, y una vez unificado el idioma funciona basttante mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aplicamos stemming, lematización y la combinación de ambos, y de la misma forma que anteriormente, no se obtiene mejora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [4  1  4  4  0  0  0  3  1  1  1  1  1  2  2  2  2  0  0  0  0  1]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.722117202268\n",
    "\n",
    "\n",
    "test:       [4  3  4  4  0  0  0  2  3  3  3  3  3  1  1  1  1  0  0  0  0  3]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.722117202268\n",
    "\n",
    "\n",
    "test:       [4  1  4  4  0  0  0  3  1  1  1  1  1  2  2  2  2  0  0  0  0  1]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.722117202268"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finalmente realizamos la prueba con ENR, y nuevamente se muestra como el método que mejores resultados da:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test:       [2  0  2  2  1  4  4  1  0  0  0  0  0  3  3  3  3  1  4  2  4  0]\n",
    "reference:  [0, 5, 0, 0, 0, 2, 2, 3, 5, 5, 5, 5, 5, 4, 4, 4, 4, 3, 2, 0, 2, 5]\n",
    "rand_score:  0.914285714286"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
